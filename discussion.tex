Our data access setup is involved and simplifying it is desirable. However,
due to our overall approach,
we have limited control over the data collection and synchronization process.
The proprietary nature of the Huawei smartwatches and Huawei Health Kit means
that data collection and synchronization are dictated by the device
manufacturer. In the future, we aim for a more open data access setup,
affording us greater control over these crucial processes.

In the realm of on-device training on iOS using \fedkit,
Core ML poses notable restrictions.
It only supports updating parameters of fully-connected layers and convolutional
layers, limiting the range of trainable model types. Furthermore,
the loss functions available in Core ML models,
limited to cross-entropy loss or mean squared error loss,
constrain the flexibility of the Core ML Trainer. We also encountered a bug
causing crashes during training with models using cross-entropy loss,
which has been reported to Apple.

Considering these challenges, we explore future directions,
including the potential replacement of Core ML with a more open solution.
Microsoft's ONNX Runtime~\cite{onnxruntime} emerges as a promising alternative,
given its active open-source development and support for training on both iOS
and Android. However, ONNX Runtime can only leverage the CPU for iOS training,
which might introduce trade-offs in terms of training acceleration.
These trade-offs might be acceptable for some applications,
but ultimately may not be addressable due to Apple's restrictions on access to
programming interfaces for its hardware.

Security considerations are paramount,
and we can enhance our system by implementing techniques both on the federated
learning layer and the communication layer.
Demonstrated practical attacks against federated learning that reverse engineer
training data emphasize the need for robust security
measures~\cite{sun2019really}. To address this,
federated learning frameworks often implement techniques like homomorphic
encryption (HE)~\cite{wang2020homo},
secure multi-party computing (SMC)~\cite{bonawitz2016practical},
and differential privacy
(DP)~\cite{dwork2006differential,geyer2017differentially}.
These mechanisms contribute to bolstering anonymity and mitigating the risk of
data-driven attacks,
and can be integrated well into our system because of our remote procedure call
approach. On the communication layer,
we could also enable Transport Layer Security (TLS)
in our communication protocols including gRPC and HTTP. These measures would
further enhance the security posture of The \fedcampus Application.

The inherent heterogeneity of \fedcampus' client devices and data suggests us to
adopt a more tailored scheduling strategies for federated learning.
In this scenario, a small number of clients may lag significantly behind others,
ultimately becoming the bottleneck for the overall training
process~\cite{chen2020asynchronous,zheng2017asynchronous}.
Although we have not observed this issue in our experiments,
likely because of the relatively small number of clients,
we could employ scheduling strategies tailored for heterogeneous devices.
These strategies sample clients each round based on their properties,
and may sample different numbers of clients each
round~\cite[e.g.,][]{zhang2022fedada,karimireddy2019error,reddi2020adaptive,luo2021cost};
they may allow clients to train for different numbers of local epochs based on
their computational capabilities~\cite{li2020federated}; or,
they may adopt an asynchronous
approach~\cite{chilimbi2014project,zhu2022online,huba2022papaya,sun2022fedsea}.
We could adopt some of these existing strategies and compare their performance,
or even develop new strategies that are more tailored to our specific scenario.

Optimizing data transfer is another avenue for improvement. Currently,
our system sends all model parameters between the server and clients each round,
potentially leading to inefficiencies over mobile networks.
Strategies such as transmitting only the difference between local and global
models\footnote{\url{
        https://ai.googleblog.com/2017/04/federated-learning-collaborative.html
    }.} or leveraging sub-networks~\cite{li2021hermes}
    can minimize data transfer.
However,
the complexity these algorithms introduced on the client side may be undesirably
challenging.
